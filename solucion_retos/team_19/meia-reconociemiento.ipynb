{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Semantic Segmentation Using HAGDAVS Dataset**","metadata":{}},{"cell_type":"markdown","source":"### **Import Libraries**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-io -y\n!pip install tensorflow\n!pip install --no-deps tensorflow-io","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:38:21.609241Z","iopub.execute_input":"2023-06-28T22:38:21.610027Z","iopub.status.idle":"2023-06-28T22:39:11.871602Z","shell.execute_reply.started":"2023-06-28T22:38:21.609992Z","shell.execute_reply":"2023-06-28T22:39:11.870202Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_io as tfio\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import TensorBoard\nimport pathlib\nfrom PIL import Image\nimport shutil\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom tensorflow.data import AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:11.874741Z","iopub.execute_input":"2023-06-28T22:39:11.875138Z","iopub.status.idle":"2023-06-28T22:39:11.884039Z","shell.execute_reply.started":"2023-06-28T22:39:11.875096Z","shell.execute_reply":"2023-06-28T22:39:11.883051Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## **Load Data**","metadata":{}},{"cell_type":"markdown","source":"This code performs the following actions:\n\n1. Defines directory paths for the input data, including the folder containing RGB images and the folder containing masks.\n2. Sets the destination folder path to save the modified images.\n3. Creates the new destination folder if it doesn't exist.\n4. Retrieves the list of file names in the masks folder.\n5. Iterates over each file in the list and does the following:\n   - Checks if the file name contains the text \"MClass\".\n   - If it does, modifies the file name by replacing \"MClass\" with \"RGB\".\n   - Creates an old file path and a new file path.\n   - Copies the old file to the new path.\n\nIn summary, the code takes masks in the \"MClass\" format and copies them to a new folder, but changes their names to the \"RGB\" format.","metadata":{}},{"cell_type":"code","source":"data_dir = pathlib.Path('/kaggle/input/hagdavs/HAGDAVS')\nimages_dir = data_dir / 'RGB'\nmasks_dir = data_dir / 'MASK'\n\nfolder_path = masks_dir  # Ruta de la carpeta que contiene las imágenes\nnew_folder_path = '/kaggle/working/semantic-segmentation-hagdavs/HAGDAVS/MASK'\n\n# Crear la nueva carpeta si no existe\nos.makedirs(new_folder_path, exist_ok=True)\n\n# Obtener la lista de nombres de archivo en la carpeta\nfile_list = os.listdir(folder_path)\n\n# Aplicar la modificación del nombre y crear nuevos archivos en la nueva carpeta\nfor filename in file_list:\n    if \"MClass\" in filename:\n        new_filename = filename.replace(\"MClass\", \"RGB\")\n        old_path = os.path.join(folder_path, filename)\n        new_path = os.path.join(new_folder_path, new_filename)\n        shutil.copy2(old_path, new_path)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:11.885489Z","iopub.execute_input":"2023-06-28T22:39:11.886126Z","iopub.status.idle":"2023-06-28T22:39:17.729213Z","shell.execute_reply.started":"2023-06-28T22:39:11.886093Z","shell.execute_reply":"2023-06-28T22:39:17.728000Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"This code performs the following actions:\n\n1. Defines directory paths for the image and mask folders.\n2. Retrieves a list of file paths for the images and masks.\n3. Creates empty lists to store image and mask patches.\n4. Sets the desired patch size.\n5. Defines a function to load and split the images and masks into patches.\n   - It reads the image and mask files.\n   - Removes the alpha channel from the images and masks.\n   - Divides the image into patches based on the specified patch size.\n   - Checks if all pixels in a mask patch are black, and if so, skips it.\n   - Appends the image and mask patches to their respective lists.\n6. Applies the load_and_split_patches function to each image and mask pair.\n7. Converts the lists of patches into TensorFlow tensors.\n8. Creates a dataset from the image and mask patches.\n\nIn summary, the code loads image and mask files, divides them into patches of a specified size, filters out patches with all-black masks, and creates a dataset for further processing or training.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_io as tfio\nimport pathlib\n\n# Ruta a las carpetas de imágenes y máscaras\nima_dir = pathlib.Path('/kaggle/input/hagdavs/HAGDAVS')\nimages_dir = ima_dir / 'RGB'\nm_dir = pathlib.Path('/kaggle/working/semantic-segmentation-hagdavs/HAGDAVS')\nmask_dir = m_dir / 'MASK'\n\n# Obtener una lista de rutas de archivo para imágenes y máscaras\nimage_paths = sorted([str(path) for path in images_dir.glob('*.tif')])\nmask_paths = sorted([str(path) for path in mask_dir.glob('*.tif')])\n\n# Crear una lista para almacenar los parches de imágenes y máscaras\nimage_patches = []\nmask_patches = []\n\n# Tamaño del parche deseado\npatch_size = (256, 256)\n\n\n# Definir una función para cargar y dividir las imágenes y máscaras en parches\ndef load_and_split_patches(image_path, mask_path):\n    image = tfio.experimental.image.decode_tiff(tf.io.read_file(image_path))\n    mask = tfio.experimental.image.decode_tiff(tf.io.read_file(mask_path))\n\n    # Eliminar el canal alfa de las imágenes y máscaras\n    image = image[:, :, :3]\n    mask = mask[:, :, :3]\n\n    # Dividir la imagen en parches\n    for i in range(0, image.shape[0], patch_size[0]):\n        for j in range(0, image.shape[1], patch_size[1]):\n            patch_image = image[i:i+patch_size[0], j:j+patch_size[1], :]\n            patch_mask = mask[i:i+patch_size[0], j:j+patch_size[1], :]\n\n            # Verificar si todos los píxeles en el parche de la máscara son negros\n            if tf.reduce_all(tf.math.equal(patch_mask, [0, 0, 0])):\n                continue\n                \n            #patch_mask = convertir_colores(patch_mask)\n            image_patches.append(patch_image)\n            mask_patches.append(patch_mask)\n\n    return None\n\n# Aplicar la función de carga y división de parches a cada par de rutas de archivo\nfor image_path, mask_path in zip(image_paths, mask_paths):\n    load_and_split_patches(image_path, mask_path)\n\n# Convertir las listas de parches en tensores\nimage_patches = tf.convert_to_tensor(image_patches)\nmask_patches = tf.convert_to_tensor(mask_patches)\n\n# Crear un dataset a partir de los parches de imágenes y máscaras\ndataset = tf.data.Dataset.from_tensor_slices((image_patches, mask_patches))","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:17.736156Z","iopub.execute_input":"2023-06-28T22:39:17.736835Z","iopub.status.idle":"2023-06-28T22:39:42.362116Z","shell.execute_reply.started":"2023-06-28T22:39:17.736793Z","shell.execute_reply":"2023-06-28T22:39:42.361131Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The size of images in dataset:","metadata":{}},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:42.364304Z","iopub.execute_input":"2023-06-28T22:39:42.364915Z","iopub.status.idle":"2023-06-28T22:39:42.372972Z","shell.execute_reply.started":"2023-06-28T22:39:42.364875Z","shell.execute_reply":"2023-06-28T22:39:42.371998Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<_TensorSliceDataset element_spec=(TensorSpec(shape=(256, 256, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(256, 256, 3), dtype=tf.uint8, name=None))>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Display Data**","metadata":{}},{"cell_type":"markdown","source":"Function for show the image, mask and prediction mask","metadata":{}},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis(\"off\")\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:42.374696Z","iopub.execute_input":"2023-06-28T22:39:42.375317Z","iopub.status.idle":"2023-06-28T22:39:42.385089Z","shell.execute_reply.started":"2023-06-28T22:39:42.375283Z","shell.execute_reply":"2023-06-28T22:39:42.383955Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## **Using only one class**","metadata":{}},{"cell_type":"markdown","source":"This code defines a function called `convertir_mascara` and applies it to a dataset using the `map` function.\n\nThe `convertir_mascara` function performs the following actions:\n\n1. Casts the input mask tensor to `float32` data type.\n2. Creates a new mask tensor filled with zeros, with dimensions `(256, 256, 1)`.\n3. Assigns values corresponding to each class in the mask:\n   - If all RGB values in the mask are `[0, 0, 0]`, assigns `0.0` to the corresponding pixel in the converted mask.\n   - If all RGB values are `[255, 0, 0]`, assigns `0.0`.\n   - If all RGB values are `[0, 255, 0]`, assigns `1.0`.\n   - If all RGB values are `[0, 0, 255]`, assigns `0.0`.\n4. Returns the converted mask.\n\nThe code then applies the `convertir_mascara` function to each element in the `dataset` using the `map` function. The `map` function takes a lambda function that applies the conversion function to each `(image, mask)` pair in the dataset, resulting in a new dataset named `mapped_dataset`. The images in the dataset remain unchanged, while the masks are converted using the `convertir_mascara` function.","metadata":{}},{"cell_type":"code","source":"def convertir_mascara(mascara):\n    mascara = tf.cast(mascara, dtype=tf.float32)\n    mascara_convertida = tf.zeros((256, 256, 1), dtype=tf.float32)\n\n    # Asigna valores correspondientes a cada clase\n    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [255, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 255, 0]), axis=-1, keepdims=True), 1.0, mascara_convertida)\n    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 255]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n\n    return mascara_convertida\n\nmapped_dataset = dataset.map(lambda x, y: (x, convertir_mascara(y)))","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:42.386622Z","iopub.execute_input":"2023-06-28T22:39:42.386975Z","iopub.status.idle":"2023-06-28T22:39:42.570827Z","shell.execute_reply.started":"2023-06-28T22:39:42.386924Z","shell.execute_reply":"2023-06-28T22:39:42.569864Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"This code defines a function called `filter_func` that filters the `mapped_dataset` based on certain criteria using the `filter` function.\n\nThe `filter_func` function performs the following actions:\n\n1. Reshapes the mask tensor into a 1-dimensional tensor.\n2. Uses `tf.unique` to obtain the unique classes present in the mask.\n3. Checks if the number of unique classes (`tf.size(unique_classes)`) is greater than or equal to 2.\n   - If there are two or more unique classes, it returns `True`, indicating that the image and mask pair should be included in the filtered dataset.\n   - If there are fewer than two unique classes, it returns `False`, indicating that the image and mask pair should be filtered out.\n\nThe code then applies the `filter_func` function to each element in the `mapped_dataset` using the `filter` function. The `filter` function takes the lambda function `filter_func` as an argument and returns a new dataset named `filtered_dataset` that contains only the image and mask pairs that satisfy the filtering criteria.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef filter_func(image, mask):\n    unique_classes = tf.unique(tf.reshape(mask, [-1]))[0]\n    return tf.size(unique_classes) >= 2\n\nfiltered_dataset = mapped_dataset.filter(filter_func)\n\ndataset_length = 0\nfor _ in filtered_dataset:\n    dataset_length += 1\n\nprint(\"Longitud aproximada del dataset filtrado:\", dataset_length)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:42.572310Z","iopub.execute_input":"2023-06-28T22:39:42.572657Z","iopub.status.idle":"2023-06-28T22:39:51.658139Z","shell.execute_reply.started":"2023-06-28T22:39:42.572624Z","shell.execute_reply":"2023-06-28T22:39:51.657104Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Longitud aproximada del dataset filtrado: 1043\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Splitting data**","metadata":{}},{"cell_type":"markdown","source":"train/test/val split","metadata":{}},{"cell_type":"code","source":"# dividir el dataset en conjuntos de entrenamiento, validación y prueba\ntotal_samples = (dataset_length)\ntrain_size = int(0.7 * total_samples)\nval_size = int(0.15 * total_samples)\ntest_size = total_samples - train_size - val_size\n\ntrain_dataset = filtered_dataset.take(train_size)\nval_dataset = filtered_dataset.skip(train_size).take(val_size)\ntest_dataset = filtered_dataset.skip(train_size + val_size).take(test_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:51.659462Z","iopub.execute_input":"2023-06-28T22:39:51.659825Z","iopub.status.idle":"2023-06-28T22:39:51.671272Z","shell.execute_reply.started":"2023-06-28T22:39:51.659789Z","shell.execute_reply":"2023-06-28T22:39:51.670392Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nBUFFER_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:51.675522Z","iopub.execute_input":"2023-06-28T22:39:51.675880Z","iopub.status.idle":"2023-06-28T22:39:51.679919Z","shell.execute_reply.started":"2023-06-28T22:39:51.675854Z","shell.execute_reply":"2023-06-28T22:39:51.678878Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidation_batches = test_dataset.batch(BATCH_SIZE)\ntest_batches = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:51.681288Z","iopub.execute_input":"2023-06-28T22:39:51.681912Z","iopub.status.idle":"2023-06-28T22:39:51.710377Z","shell.execute_reply.started":"2023-06-28T22:39:51.681879Z","shell.execute_reply":"2023-06-28T22:39:51.709518Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## **Unet**","metadata":{}},{"cell_type":"code","source":"def double_conv_block(x, n_filters):\n\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # dropout\n    x = layers.Dropout(0.2)(x)\n\n    return x\n\ndef downsample_block(x, n_filters):\n    f = double_conv_block(x, n_filters)\n    p = layers.MaxPool2D(2)(f)\n    p = layers.Dropout(0.2)(p)\n\n    return f, p\n\ndef upsample_block(x, conv_features, n_filters):\n    # upsample\n    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n    # concatenate \n    x = layers.concatenate([x, conv_features])\n    # dropout\n    x = layers.Dropout(0.2)(x)\n    # Conv2D twice with ReLU activation\n    x = double_conv_block(x, n_filters)\n\n    return x\n    \ndef build_unet_model():\n\n    # inputs\n    inputs = layers.Input(shape=(256,256,3))\n\n    # encoder: contracting path - downsample\n    # 1 - downsample\n    f1, p1 = downsample_block(inputs, 64)\n    # 2 - downsample\n    f2, p2 = downsample_block(p1, 128)\n    # 3 - downsample\n    f3, p3 = downsample_block(p2, 256)\n    # 4 - downsample\n    f4, p4 = downsample_block(p3, 512)\n\n    # 5 - bottleneck\n    bottleneck = double_conv_block(p4, 1024)\n    bottleneck = layers.Dropout(0.3)(bottleneck)\n\n    # decoder: expanding path - upsample\n    # 6 - upsample\n    u6 = upsample_block(bottleneck, f4, 512)\n    # 7 - upsample\n    u7 = upsample_block(u6, f3, 256)\n    # 8 - upsample\n    u8 = upsample_block(u7, f2, 128)\n    # 9 - upsample\n    u9 = upsample_block(u8, f1, 64)\n\n    # outputs\n    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n\n    # unet model with Keras Functional API\n    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n\n    return unet_model\n\nunet_model = build_unet_model()\n\n#loss = keras.losses.sparse_categorical_crossentropy()\n\nunet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n                   loss='binary_crossentropy',\n                   metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:51.712006Z","iopub.execute_input":"2023-06-28T22:39:51.712372Z","iopub.status.idle":"2023-06-28T22:39:52.488360Z","shell.execute_reply.started":"2023-06-28T22:39:51.712316Z","shell.execute_reply":"2023-06-28T22:39:52.487381Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## **Training**","metadata":{}},{"cell_type":"code","source":"keras.backend.clear_session()\nNUM_EPOCHS = 18\nBATCH_SIZE = 16\n\n\nSTEPS_PER_EPOCH = total_samples // BATCH_SIZE\n\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n\nmodel_history = unet_model.fit(train_batches,\n                               epochs=NUM_EPOCHS,\n                               steps_per_epoch=STEPS_PER_EPOCH,\n                               validation_steps=VALIDATION_STEPS,\n                               validation_data=validation_batches,)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T22:39:52.489830Z","iopub.execute_input":"2023-06-28T22:39:52.490229Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"2023-06-28 22:40:00.900317: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inU-Net/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"65/65 [==============================] - 145s 2s/step - loss: 0.2281 - accuracy: 0.9467 - val_loss: 193.1505 - val_accuracy: 0.7711\nEpoch 2/10\n65/65 [==============================] - 125s 2s/step - loss: 0.1194 - accuracy: 0.9637 - val_loss: 0.6277 - val_accuracy: 0.4940\nEpoch 3/10\n65/65 [==============================] - 124s 2s/step - loss: 0.0978 - accuracy: 0.9680 - val_loss: 0.7837 - val_accuracy: 0.8149\nEpoch 4/10\n65/65 [==============================] - 125s 2s/step - loss: 0.0895 - accuracy: 0.9688 - val_loss: 0.5928 - val_accuracy: 0.7385\nEpoch 5/10\n65/65 [==============================] - 121s 2s/step - loss: 0.0917 - accuracy: 0.9673 - val_loss: 0.4317 - val_accuracy: 0.7221\nEpoch 6/10\n 4/65 [>.............................] - ETA: 1:47 - loss: 0.0710 - accuracy: 0.9741","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Visualization of predictions**","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask, threshold=0.2):\n  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n  return pred_mask[0]\n\ndef show_predictions(dataset, model, num):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_batches, unet_model, 16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}