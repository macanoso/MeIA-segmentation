{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac04ff68",
   "metadata": {
    "papermill": {
     "duration": 0.009055,
     "end_time": "2023-06-28T21:31:01.160883",
     "exception": false,
     "start_time": "2023-06-28T21:31:01.151828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Proyecto Final MeIA. Segmentación Semántica con dataset HAGDAVS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3945a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:31:01.179965Z",
     "iopub.status.busy": "2023-06-28T21:31:01.179227Z",
     "iopub.status.idle": "2023-06-28T21:32:33.708670Z",
     "shell.execute_reply": "2023-06-28T21:32:33.707381Z"
    },
    "papermill": {
     "duration": 92.542201,
     "end_time": "2023-06-28T21:32:33.711757",
     "exception": false,
     "start_time": "2023-06-28T21:31:01.169556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall tensorflow -y\n",
    "!pip uninstall tensorflow-io -y\n",
    "!pip install tensorflow\n",
    "!pip install --no-deps tensorflow-io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd251c6",
   "metadata": {
    "papermill": {
     "duration": 0.008223,
     "end_time": "2023-06-28T21:32:33.728786",
     "exception": false,
     "start_time": "2023-06-28T21:32:33.720563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Carga de librerías*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80c5f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:32:33.749160Z",
     "iopub.status.busy": "2023-06-28T21:32:33.747572Z",
     "iopub.status.idle": "2023-06-28T21:32:42.995535Z",
     "shell.execute_reply": "2023-06-28T21:32:42.994248Z"
    },
    "papermill": {
     "duration": 9.26092,
     "end_time": "2023-06-28T21:32:42.998215",
     "exception": false,
     "start_time": "2023-06-28T21:32:33.737295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.data import AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284424e",
   "metadata": {
    "papermill": {
     "duration": 0.008897,
     "end_time": "2023-06-28T21:32:43.016277",
     "exception": false,
     "start_time": "2023-06-28T21:32:43.007380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee393eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:32:43.036543Z",
     "iopub.status.busy": "2023-06-28T21:32:43.035142Z",
     "iopub.status.idle": "2023-06-28T21:32:50.409350Z",
     "shell.execute_reply": "2023-06-28T21:32:50.408357Z"
    },
    "papermill": {
     "duration": 7.386596,
     "end_time": "2023-06-28T21:32:50.411762",
     "exception": false,
     "start_time": "2023-06-28T21:32:43.025166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/kaggle/input/hagdavs/HAGDAVS')\n",
    "images_dir = data_dir / 'RGB'\n",
    "masks_dir = data_dir / 'MASK'\n",
    "\n",
    "folder_path = masks_dir  # Ruta de la carpeta que contiene las imágenes\n",
    "new_folder_path = '/kaggle/working/semantic-segmentation-hagdavs/HAGDAVS/MASK'\n",
    "\n",
    "# Crear la nueva carpeta si no existe\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "# Obtener la lista de nombres de archivo en la carpeta\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Aplicar la modificación del nombre y crear nuevos archivos en la nueva carpeta\n",
    "for filename in file_list:\n",
    "    if \"MClass\" in filename:\n",
    "        new_filename = filename.replace(\"MClass\", \"RGB\")\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        new_path = os.path.join(new_folder_path, new_filename)\n",
    "        shutil.copy2(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360c10c",
   "metadata": {
    "papermill": {
     "duration": 0.008222,
     "end_time": "2023-06-28T21:32:50.428305",
     "exception": false,
     "start_time": "2023-06-28T21:32:50.420083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This code performs the following actions:\n",
    "\n",
    "1. Defines directory paths for the image and mask folders.\n",
    "2. Retrieves a list of file paths for the images and masks.\n",
    "3. Creates empty lists to store image and mask patches.\n",
    "4. Sets the desired patch size.\n",
    "5. Defines a function to load and split the images and masks into patches.\n",
    "   - It reads the image and mask files.\n",
    "   - Removes the alpha channel from the images and masks.\n",
    "   - Divides the image into patches based on the specified patch size.\n",
    "   - Checks if all pixels in a mask patch are black, and if so, skips it.\n",
    "   - Appends the image and mask patches to their respective lists.\n",
    "6. Applies the load_and_split_patches function to each image and mask pair.\n",
    "7. Converts the lists of patches into TensorFlow tensors.\n",
    "8. Creates a dataset from the image and mask patches.\n",
    "\n",
    "In summary, the code loads image and mask files, divides them into patches of a specified size, filters out patches with all-black masks, and creates a dataset for further processing or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fb6122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:32:50.446261Z",
     "iopub.status.busy": "2023-06-28T21:32:50.445932Z",
     "iopub.status.idle": "2023-06-28T21:33:24.265929Z",
     "shell.execute_reply": "2023-06-28T21:33:24.264927Z"
    },
    "papermill": {
     "duration": 33.832052,
     "end_time": "2023-06-28T21:33:24.268366",
     "exception": false,
     "start_time": "2023-06-28T21:32:50.436314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import pathlib\n",
    "\n",
    "# Ruta a las carpetas de imágenes y máscaras\n",
    "ima_dir = pathlib.Path('/kaggle/input/hagdavs/HAGDAVS')\n",
    "images_dir = ima_dir / 'RGB'\n",
    "m_dir = pathlib.Path('/kaggle/working/semantic-segmentation-hagdavs/HAGDAVS')\n",
    "mask_dir = m_dir / 'MASK'\n",
    "\n",
    "# Obtener una lista de rutas de archivo para imágenes y máscaras\n",
    "image_paths = sorted([str(path) for path in images_dir.glob('*.tif')])\n",
    "mask_paths = sorted([str(path) for path in mask_dir.glob('*.tif')])\n",
    "\n",
    "# Crear una lista para almacenar los parches de imágenes y máscaras\n",
    "image_patches = []\n",
    "mask_patches = []\n",
    "\n",
    "# Tamaño del parche deseado\n",
    "patch_size = (256, 256)\n",
    "\n",
    "\n",
    "# Definir una función para cargar y dividir las imágenes y máscaras en parches\n",
    "def load_and_split_patches(image_path, mask_path):\n",
    "    image = tfio.experimental.image.decode_tiff(tf.io.read_file(image_path))\n",
    "    mask = tfio.experimental.image.decode_tiff(tf.io.read_file(mask_path))\n",
    "\n",
    "    # Eliminar el canal alfa de las imágenes y máscaras\n",
    "    image = image[:, :, :3]\n",
    "    mask = mask[:, :, :3]\n",
    "\n",
    "    # Dividir la imagen en parches\n",
    "    for i in range(0, image.shape[0], patch_size[0]):\n",
    "        for j in range(0, image.shape[1], patch_size[1]):\n",
    "            patch_image = image[i:i+patch_size[0], j:j+patch_size[1], :]\n",
    "            patch_mask = mask[i:i+patch_size[0], j:j+patch_size[1], :]\n",
    "\n",
    "            # Verificar si todos los píxeles en el parche de la máscara son negros\n",
    "            if tf.reduce_all(tf.math.equal(patch_mask, [0, 0, 0])):\n",
    "                continue\n",
    "                \n",
    "            #patch_mask = convertir_colores(patch_mask)\n",
    "            image_patches.append(patch_image)\n",
    "            mask_patches.append(patch_mask)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Aplicar la función de carga y división de parches a cada par de rutas de archivo\n",
    "for image_path, mask_path in zip(image_paths, mask_paths):\n",
    "    load_and_split_patches(image_path, mask_path)\n",
    "\n",
    "# Convertir las listas de parches en tensores\n",
    "image_patches = tf.convert_to_tensor(image_patches)\n",
    "mask_patches = tf.convert_to_tensor(mask_patches)\n",
    "\n",
    "# Crear un dataset a partir de los parches de imágenes y máscaras\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbbf98",
   "metadata": {
    "papermill": {
     "duration": 0.008124,
     "end_time": "2023-06-28T21:33:24.285190",
     "exception": false,
     "start_time": "2023-06-28T21:33:24.277066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The size of images in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40462bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:33:24.303685Z",
     "iopub.status.busy": "2023-06-28T21:33:24.303298Z",
     "iopub.status.idle": "2023-06-28T21:33:24.902924Z",
     "shell.execute_reply": "2023-06-28T21:33:24.901706Z"
    },
    "papermill": {
     "duration": 0.611562,
     "end_time": "2023-06-28T21:33:24.904971",
     "exception": true,
     "start_time": "2023-06-28T21:33:24.293409",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_patches))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mask_patches))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(len(image_patches))\n",
    "print(len(mask_patches))\n",
    "print(type(image_patches[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b979f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Se imprimen unas imagenes para conocer el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f14e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T20:06:32.213981Z",
     "iopub.status.busy": "2023-06-28T20:06:32.213580Z",
     "iopub.status.idle": "2023-06-28T20:06:32.782546Z",
     "shell.execute_reply": "2023-06-28T20:06:32.781659Z",
     "shell.execute_reply.started": "2023-06-28T20:06:32.213948Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "imagen=image_patches[-2]\n",
    "mask=mask_patches[-2]\n",
    "plt.imshow(imagen)\n",
    "plt.show()\n",
    "plt.imshow(mask)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a637280",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Aumento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab5e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:20:16.566111Z",
     "iopub.status.busy": "2023-06-28T21:20:16.565125Z",
     "iopub.status.idle": "2023-06-28T21:20:21.860519Z",
     "shell.execute_reply": "2023-06-28T21:20:21.859541Z",
     "shell.execute_reply.started": "2023-06-28T21:20:16.566075Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Se decidió expandir 184 imagenes para dejar el tamaño del ds en 1500\n",
    "exp_img=image_patches[:184]\n",
    "exp_mask=mask_patches[:184]\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.Rescaling(1.0 / 255.0),\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "])\n",
    "\n",
    "\n",
    "img_transf = []\n",
    "mask_transf = []\n",
    "\n",
    "for imagen, mascara in zip(exp_img, exp_mask):\n",
    "    imagen_transformada = data_augmentation(tf.expand_dims(imagen, axis=0))[0]\n",
    "    mascara_transformada = data_augmentation(tf.expand_dims(mascara, axis=0))[0]\n",
    "    img_transf.append(imagen_transformada)\n",
    "    mask_transf.append(mascara_transformada)\n",
    "\n",
    "plt.imshow(img_transf[0])\n",
    "plt.show()\n",
    "plt.imshow(mask_transf[0])\n",
    "plt.show()\n",
    "plt.imshow(image_patches[0])\n",
    "plt.show()\n",
    "plt.imshow(mask_patches[0])\n",
    "plt.show()\n",
    "\n",
    "image_patches=tf.concat([image_patches,img_transf],axis=0)\n",
    "mask_patches=tf.concat([mask_patches,mask_transf],axis=0)\n",
    "    \n",
    "print(len(image_patches))\n",
    "print(len(mask_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3ec5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T21:29:03.934181Z",
     "iopub.status.busy": "2023-06-28T21:29:03.933178Z",
     "iopub.status.idle": "2023-06-28T21:29:03.943948Z",
     "shell.execute_reply": "2023-06-28T21:29:03.942758Z",
     "shell.execute_reply.started": "2023-06-28T21:29:03.934148Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_patches, mask_patches))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ced3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Display Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba097dd6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Function for show the image, mask and prediction mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88fa4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:38:55.740539Z",
     "iopub.status.busy": "2023-06-28T01:38:55.739248Z",
     "iopub.status.idle": "2023-06-28T01:38:55.753729Z",
     "shell.execute_reply": "2023-06-28T01:38:55.752117Z",
     "shell.execute_reply.started": "2023-06-28T01:38:55.740497Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c94e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Using only one class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575b696",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This code defines a function called `convertir_mascara` and applies it to a dataset using the `map` function.\n",
    "\n",
    "The `convertir_mascara` function performs the following actions:\n",
    "\n",
    "1. Casts the input mask tensor to `float32` data type.\n",
    "2. Creates a new mask tensor filled with zeros, with dimensions `(256, 256, 1)`.\n",
    "3. Assigns values corresponding to each class in the mask:\n",
    "   - If all RGB values in the mask are `[0, 0, 0]`, assigns `0.0` to the corresponding pixel in the converted mask.\n",
    "   - If all RGB values are `[255, 0, 0]`, assigns `0.0`.\n",
    "   - If all RGB values are `[0, 255, 0]`, assigns `1.0`.\n",
    "   - If all RGB values are `[0, 0, 255]`, assigns `0.0`.\n",
    "4. Returns the converted mask.\n",
    "\n",
    "The code then applies the `convertir_mascara` function to each element in the `dataset` using the `map` function. The `map` function takes a lambda function that applies the conversion function to each `(image, mask)` pair in the dataset, resulting in a new dataset named `mapped_dataset`. The images in the dataset remain unchanged, while the masks are converted using the `convertir_mascara` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc964c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:38:55.756138Z",
     "iopub.status.busy": "2023-06-28T01:38:55.755514Z",
     "iopub.status.idle": "2023-06-28T01:38:56.124655Z",
     "shell.execute_reply": "2023-06-28T01:38:56.122584Z",
     "shell.execute_reply.started": "2023-06-28T01:38:55.756105Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convertir_mascara(mascara):\n",
    "    mascara = tf.cast(mascara, dtype=tf.float32)\n",
    "    mascara_convertida = tf.zeros((256, 256, 1), dtype=tf.float32)\n",
    "\n",
    "    # Asigna valores correspondientes a cada clase\n",
    "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
    "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [255, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
    "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 255, 0]), axis=-1, keepdims=True), 1.0, mascara_convertida)\n",
    "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 255]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
    "\n",
    "    return mascara_convertida\n",
    "\n",
    "mapped_dataset = dataset.map(lambda x, y: (x, convertir_mascara(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ad94e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This code defines a function called `filter_func` that filters the `mapped_dataset` based on certain criteria using the `filter` function.\n",
    "\n",
    "The `filter_func` function performs the following actions:\n",
    "\n",
    "1. Reshapes the mask tensor into a 1-dimensional tensor.\n",
    "2. Uses `tf.unique` to obtain the unique classes present in the mask.\n",
    "3. Checks if the number of unique classes (`tf.size(unique_classes)`) is greater than or equal to 2.\n",
    "   - If there are two or more unique classes, it returns `True`, indicating that the image and mask pair should be included in the filtered dataset.\n",
    "   - If there are fewer than two unique classes, it returns `False`, indicating that the image and mask pair should be filtered out.\n",
    "\n",
    "The code then applies the `filter_func` function to each element in the `mapped_dataset` using the `filter` function. The `filter` function takes the lambda function `filter_func` as an argument and returns a new dataset named `filtered_dataset` that contains only the image and mask pairs that satisfy the filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6dd66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:38:56.127076Z",
     "iopub.status.busy": "2023-06-28T01:38:56.126535Z",
     "iopub.status.idle": "2023-06-28T01:39:07.590846Z",
     "shell.execute_reply": "2023-06-28T01:39:07.589493Z",
     "shell.execute_reply.started": "2023-06-28T01:38:56.127031Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def filter_func(image, mask):\n",
    "    unique_classes = tf.unique(tf.reshape(mask, [-1]))[0]\n",
    "    return tf.size(unique_classes) >= 2\n",
    "\n",
    "filtered_dataset = mapped_dataset.filter(filter_func)\n",
    "\n",
    "dataset_length = 0\n",
    "for _ in filtered_dataset:\n",
    "    dataset_length += 1\n",
    "\n",
    "print(\"Longitud aproximada del dataset filtrado:\", dataset_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5be9bb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Splitting data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e960fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "train/test/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584f919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:39:07.593238Z",
     "iopub.status.busy": "2023-06-28T01:39:07.592548Z",
     "iopub.status.idle": "2023-06-28T01:39:07.614745Z",
     "shell.execute_reply": "2023-06-28T01:39:07.613634Z",
     "shell.execute_reply.started": "2023-06-28T01:39:07.593090Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dividir el dataset en conjuntos de entrenamiento, validación y prueba\n",
    "total_samples = (dataset_length)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.15 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "train_dataset = filtered_dataset.take(train_size)\n",
    "val_dataset = filtered_dataset.skip(train_size).take(val_size)\n",
    "test_dataset = filtered_dataset.skip(train_size + val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6357e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:39:07.621262Z",
     "iopub.status.busy": "2023-06-28T01:39:07.620597Z",
     "iopub.status.idle": "2023-06-28T01:39:07.627942Z",
     "shell.execute_reply": "2023-06-28T01:39:07.626648Z",
     "shell.execute_reply.started": "2023-06-28T01:39:07.621224Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f70fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:39:07.630935Z",
     "iopub.status.busy": "2023-06-28T01:39:07.630060Z",
     "iopub.status.idle": "2023-06-28T01:39:07.669551Z",
     "shell.execute_reply": "2023-06-28T01:39:07.667343Z",
     "shell.execute_reply.started": "2023-06-28T01:39:07.630888Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_batches = test_dataset.batch(BATCH_SIZE)\n",
    "test_batches = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fa8bb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Unet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27412760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T01:39:07.672668Z",
     "iopub.status.busy": "2023-06-28T01:39:07.672086Z",
     "iopub.status.idle": "2023-06-28T01:39:09.678214Z",
     "shell.execute_reply": "2023-06-28T01:39:09.676646Z",
     "shell.execute_reply.started": "2023-06-28T01:39:07.672620Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "\n",
    "    # Conv2D then ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    # Conv2D then ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    # Conv2D then ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    # dropout\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    p = layers.Dropout(0.2)(p)\n",
    "\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # upsample\n",
    "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    # concatenate \n",
    "    x = layers.concatenate([x, conv_features])\n",
    "    # dropout\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # Conv2D twice with ReLU activation\n",
    "    x = double_conv_block(x, n_filters)\n",
    "\n",
    "    return x\n",
    "    \n",
    "def build_unet_model():\n",
    "\n",
    "    # inputs\n",
    "    inputs = layers.Input(shape=(256,256,3))\n",
    "\n",
    "    # encoder: contracting path - downsample\n",
    "    # 1 - downsample\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    # 2 - downsample\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    # 3 - downsample\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    # 4 - downsample\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "    # 5 - bottleneck\n",
    "    bottleneck = double_conv_block(p4, 1024)\n",
    "    bottleneck = layers.Dropout(0.3)(bottleneck)\n",
    "\n",
    "    # decoder: expanding path - upsample\n",
    "    # 6 - upsample\n",
    "    u6 = upsample_block(bottleneck, f4, 512)\n",
    "    # 7 - upsample\n",
    "    u7 = upsample_block(u6, f3, 256)\n",
    "    # 8 - upsample\n",
    "    u8 = upsample_block(u7, f2, 128)\n",
    "    # 9 - upsample\n",
    "    u9 = upsample_block(u8, f1, 64)\n",
    "\n",
    "    # outputs\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n",
    "\n",
    "    # unet model with Keras Functional API\n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model\n",
    "\n",
    "unet_model = build_unet_model()\n",
    "\n",
    "#loss = keras.losses.sparse_categorical_crossentropy()\n",
    "\n",
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8765ff5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fd583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-28T02:51:05.623940Z",
     "iopub.status.busy": "2023-06-28T02:51:05.623079Z",
     "iopub.status.idle": "2023-06-28T02:51:06.142560Z",
     "shell.execute_reply": "2023-06-28T02:51:06.139201Z",
     "shell.execute_reply.started": "2023-06-28T02:51:05.623905Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "STEPS_PER_EPOCH = total_samples // BATCH_SIZE\n",
    "\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n",
    "\n",
    "model_history = unet_model.fit(train_batches,\n",
    "                               epochs=NUM_EPOCHS,\n",
    "                               steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                               validation_steps=VALIDATION_STEPS,\n",
    "                               validation_data=validation_batches,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e966561",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Visualization of predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dceef9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask, threshold=0.2):\n",
    "  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n",
    "  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n",
    "  return pred_mask[0]\n",
    "\n",
    "def show_predictions(dataset, model, num):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5018a1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_predictions(test_batches, unet_model, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 161.844051,
   "end_time": "2023-06-28T21:33:28.167484",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-28T21:30:46.323433",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
